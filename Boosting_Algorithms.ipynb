{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a38a6315-4d06-49db-bad4-1d6e21987ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8601307999388031"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ada Boost\n",
    "import pandas as pd\n",
    "# Step-1: Data Collection\n",
    "dataset=pd.read_csv(\"insurance_pre.csv\")\n",
    "# Step2: Data pre-processing\n",
    "dataset=pd.get_dummies(dataset,drop_first=True)\n",
    "# Step 3: Input/output split\n",
    "independent=dataset[['age', 'sex_male', 'bmi', 'children', 'smoker_yes']]\n",
    "dependent=dataset[['charges']]\n",
    "# step 4: Split into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(independent, dependent,test_size=1/3,random_state=0)\n",
    "#Step 4.1: Standardizing inputs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "x_train=sc.fit_transform(x_train)\n",
    "x_test=sc.transform(x_test)\n",
    "# Step 5: Create SLR model\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "regressor = AdaBoostRegressor(estimator=None, n_estimators=100, learning_rate=0.1,loss=\"linear\", random_state=42)\n",
    "regressor=regressor.fit(x_train,y_train)\n",
    "# Step 6: Test the model\n",
    "y_pred=regressor.predict(x_test)\n",
    "# Step 7: Evaluate the model\n",
    "from sklearn.metrics import r2_score\n",
    "r_score=r2_score(y_test,y_pred)\n",
    "r_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aabdf235-44ce-43a3-8f1a-8251805daeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.3-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (from xgboost) (2.1.3)\n",
      "Requirement already satisfied: scipy in c:\\anaconda3\\lib\\site-packages (from xgboost) (1.15.3)\n",
      "Downloading xgboost-3.0.3-py3-none-win_amd64.whl (149.9 MB)\n",
      "   ---------------------------------------- 0.0/149.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 5.8/149.9 MB 38.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 12.1/149.9 MB 32.3 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 15.7/149.9 MB 26.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 26.0/149.9 MB 33.2 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 36.4/149.9 MB 36.0 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 46.4/149.9 MB 37.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 56.4/149.9 MB 39.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 67.4/149.9 MB 40.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 77.1/149.9 MB 41.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 87.0/149.9 MB 41.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 96.2/149.9 MB 41.6 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 104.3/149.9 MB 41.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 112.7/149.9 MB 41.4 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 120.8/149.9 MB 41.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 130.5/149.9 MB 41.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 140.0/149.9 MB 41.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  148.9/149.9 MB 41.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.7/149.9 MB 41.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.7/149.9 MB 41.7 MB/s eta 0:00:01\n",
      "   --------------------------------------- 149.9/149.9 MB 35.9 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e769af-4a4c-4f95-8947-4dea4e2008b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8853746652603149"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ada Boost\n",
    "import pandas as pd\n",
    "# Step-1: Data Collection\n",
    "dataset=pd.read_csv(\"insurance_pre.csv\")\n",
    "# Step2: Data pre-processing\n",
    "dataset=pd.get_dummies(dataset,drop_first=True)\n",
    "# Step 3: Input/output split\n",
    "independent=dataset[['age', 'sex_male', 'bmi', 'children', 'smoker_yes']]\n",
    "dependent=dataset[['charges']]\n",
    "# step 4: Split into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(independent, dependent,test_size=1/3,random_state=0)\n",
    "#Step 4.1: Standardizing inputs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "x_train=sc.fit_transform(x_train)\n",
    "x_test=sc.transform(x_test)\n",
    "# Step 5: Create SLR model\n",
    "from xgboost import XGBRegressor\n",
    "regressor = XGBRegressor(\n",
    "    n_estimators=100,         # number of boosting rounds\n",
    "    learning_rate=0.1,        # shrinkage step\n",
    "    max_depth=4,              # max depth of tree\n",
    "    subsample=0.8,            # % of rows to sample for each tree\n",
    "    colsample_bytree=0.8,     # % of features to use per tree\n",
    "    objective='reg:squarederror',  # loss function\n",
    "    random_state=42)\n",
    "regressor=regressor.fit(x_train,y_train)\n",
    "# Step 6: Test the model\n",
    "y_pred=regressor.predict(x_test)\n",
    "# Step 7: Evaluate the model\n",
    "from sklearn.metrics import r2_score\n",
    "r_score=r2_score(y_test,y_pred)\n",
    "r_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11650c01-0461-4728-8169-54df882c1fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\anaconda3\\lib\\site-packages (from lightgbm) (2.1.3)\n",
      "Requirement already satisfied: scipy in c:\\anaconda3\\lib\\site-packages (from lightgbm) (1.15.3)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 17.4 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ec4c4e6-7856-4ddc-82bd-be963d2899d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\Anaconda3\\Lib\\subprocess.py\", line 554, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Anaconda3\\Lib\\subprocess.py\", line 1039, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Anaconda3\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n",
      "C:\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000906 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 316\n",
      "[LightGBM] [Info] Number of data points in the train set: 892, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 13138.323530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8705258475072013"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LG Boost\n",
    "import pandas as pd\n",
    "# Step-1: Data Collection\n",
    "dataset=pd.read_csv(\"insurance_pre.csv\")\n",
    "# Step2: Data pre-processing\n",
    "dataset=pd.get_dummies(dataset,drop_first=True)\n",
    "# Step 3: Input/output split\n",
    "independent=dataset[['age', 'sex_male', 'bmi', 'children', 'smoker_yes']]\n",
    "dependent=dataset[['charges']]\n",
    "# step 4: Split into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(independent, dependent,test_size=1/3,random_state=0)\n",
    "#Step 4.1: Standardizing inputs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "x_train=sc.fit_transform(x_train)\n",
    "x_test=sc.transform(x_test)\n",
    "# Step 5: Create SLR model\n",
    "from lightgbm import LGBMRegressor\n",
    "regressor = LGBMRegressor(\n",
    "    boosting_type='gbdt',     # Gradient Boosting Decision Tree (default)\n",
    "    num_leaves=31,            # Number of leaves per tree\n",
    "    max_depth=-1,             # No limit unless specified\n",
    "    learning_rate=0.1,        # Shrinkage rate\n",
    "    n_estimators=100,         # Number of boosting iterations\n",
    "    subsample=0.8,            # Row sampling\n",
    "    colsample_bytree=0.8,     # Feature sampling\n",
    "    reg_alpha=0.0,            # L1 regularization\n",
    "    reg_lambda=0.0,           # L2 regularization\n",
    "    random_state=0\n",
    ")\n",
    "regressor=regressor.fit(x_train,y_train)\n",
    "# Step 6: Test the model\n",
    "y_pred=regressor.predict(x_test)\n",
    "# Step 7: Evaluate the model\n",
    "from sklearn.metrics import r2_score\n",
    "r_score=r2_score(y_test,y_pred)\n",
    "r_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed80eb8-f6c7-4aa5-9a80-4b283270fef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
